# TODOs

- [x] Set up a basic Litestar app with simple health check endpoint and configure the server to run with uvicorn. The app needs proper settings with environment variables for configuration, including debug mode, CORS settings, and logging levels.
- [x] Configure CORS to allow proper cross-origin requests, set up OpenAPI documentation with appropriate metadata about the API, and implement structured logging with request IDs for better traceability and debugging.
- [x] Create a well-organized project structure with separate directories for routes, services, and models. Make sure there's proper separation of concerns between different parts of the application.
- [x] Implement initial proxying capability for OpenAI chat completions endpoint, forwarding requests from clients to OpenAI and then returning the responses back to clients with minimal modification.
- [x] Set up error handling and logging that captures all relevant information about requests and responses, including request IDs in structured log output to trace request flow through the system.
- [x] Create a router for Deepgram voice agent websocket connections that can handle both incoming client connections and the proxying to Deepgram's voice agent API, properly maintaining the websocket state.
- [x] Implement WebSocket message handling for the Deepgram agent that can properly determine message types, route them correctly, and maintain the conversation flow for voice agent interactions.
- [x] Create RAG capabilities with vector store search, allowing the service to retrieve relevant context from a vector database to enhance both chat completions and voice agent responses.
- [x] Build a tools registry system for function/tool calling capabilities that allows easy registration, discovery, and execution of tool functions that can be exposed to LLMs.
- [x] Create a method in @function_calling.py that accepts an entire openai chat completion request object, and adds our built-in function calls to it (without replacing any existing function calls), returning the augmented config in turn. While we're at it, let's prefix our internal function calls with something like gnosis_function_ so they're less likely to clash with any client-provided function calls
- [x] Set up a method capable of receiving an upstream chat completion request (from the client), and returning chat completion config augmented with the configuration for the built-in tools, so that it is then sent to OpenAI's chat completion service as the user gave to us, complete with our own function calls.
- [ ] Set up another method similar to the last, capable of receiving Deepgram agent config (from the client), and returning agent config augmented with the configuration for the built-in tools, so that it is then sent on to Deepgram's agent websocket as the user gave to us, complete with our own function calls.
- [ ] Process chat completion responses for built-in tool requests. This should run the built-in tools using the tool registry, reply to the LLM, and then finally respond to the client when the chat completion returns an assistant message. Multiple tool calls can be requested at once. Some tool calls may not be built-in and these can be returned to the client, like any other message.
- [ ] Create a new function in the function calling service. It should parse agent messages, looking for built-in tool requests. Multiple tool calls can be requested at once. Some tool calls may not be built-in and these requests can be returned to the client, minus any built-in requests. We should run the built-in tools using the tool registry and send a response to the agent as soon as possible. These tool calls should be able to run in parallel, where necessary.
